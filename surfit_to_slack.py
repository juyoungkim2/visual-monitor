import os, json, time, re
import datetime as dt
from pathlib import Path
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

SURFIT_BASE = "https://www.surfit.io"
LIST_PAGES = [
    "https://www.surfit.io/",
    "https://www.surfit.io/discover",
]
WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL")
CACHE = Path(".cache/surfit_seen.json")
MAX_ITEMS = 8  # í•œ ë²ˆì— ìŠ¬ë™ìœ¼ë¡œ ë³´ë‚¼ ê°œìˆ˜(ìµœëŒ€ 8~10 ì¶”ì²œ)

def ensure_cache():
    CACHE.parent.mkdir(parents=True, exist_ok=True)
    if not CACHE.exists():
        CACHE.write_text("[]", encoding="utf-8")

def fetch_html(url, timeout=15):
    headers = {
        "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) SurfitSlackBot/1.0",
        "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8"
    }
    return requests.get(url, headers=headers, timeout=timeout)

def parse_list(html):
    """ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ /article/ ë§í¬ë“¤ íŒŒì‹±"""
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for a in soup.select("a[href*='/article/']"):
        href = a.get("href") or ""
        if "/article/" not in href:
            continue
        url = href if href.startswith("http") else urljoin(SURFIT_BASE, href)
        # ì œëª© ì¶”ì¶œ(ë§í¬ í…ìŠ¤íŠ¸ ë˜ëŠ” í•˜ìœ„ ì—˜ë¦¬ë¨¼íŠ¸)
        title = " ".join((a.get_text(" ", strip=True) or "").split())
        if not title:
            # ì¹´ë“œ êµ¬ì¡°ì¼ ë•Œ data- ì†ì„± ë“± ì‹œë„
            title = a.get("title") or ""
        if url and title:
            links.append((title, url))
    # ì¤‘ë³µ ì œê±°, ìµœì‹ ì´ ìœ„ë¡œ ì˜¤ë„ë¡ ì•ìª½ ìš°ì„ 
    seen = set()
    out = []
    for t, u in links:
        if u in seen: continue
        seen.add(u); out.append((t, u))
    return out

def parse_meta_description(html):
    soup = BeautifulSoup(html, "html.parser")
    # og:description > meta description ìˆœì„œ
    og = soup.find("meta", attrs={"property":"og:description"})
    if og and og.get("content"): return og.get("content").strip()
    md = soup.find("meta", attrs={"name":"description"})
    if md and md.get("content"): return md.get("content").strip()
    # ë³¸ë¬¸ ì²« ë¬¸ë‹¨ ëŒ€ìš©
    p = soup.find("p")
    if p: return " ".join(p.get_text(" ", strip=True).split())
    return ""

def load_seen():
    try:
        return set(json.loads(CACHE.read_text(encoding="utf-8")))
    except Exception:
        return set()

def save_seen(seen_set):
    CACHE.write_text(json.dumps(sorted(list(seen_set))[-500:],
                                ensure_ascii=False, indent=2), encoding="utf-8")

def article_id(url):
    # https://www.surfit.io/article/xxxxx í˜•íƒœì—ì„œ id ì¶”ì¶œ
    m = re.search(r"/article/([A-Za-z0-9\-\_]+)", url)
    return m.group(1) if m else url

def pick_new_items(candidates, seen):
    new = []
    for title, url in candidates:
        aid = article_id(url)
        if aid not in seen:
            new.append((title, url, aid))
    return new

def build_slack_blocks(items):
    today = dt.datetime.now(dt.timezone(dt.timedelta(hours=9))).strftime("%Y-%m-%d")
    blocks = [{
        "type":"header",
        "text":{"type":"plain_text","text":f"ğŸ§© Surfit ì‹ ê·œ ì•„í‹°í´ - {today}"}
    }]
    sess = requests.Session()
    for title, url, _ in items[:MAX_ITEMS]:
        try:
            r = fetch_html(url, timeout=12)
            desc = parse_meta_description(r.text)[:280]
        except Exception:
            desc = ""
        text = f"*<{url}|{title}>*\n{desc}" if desc else f"*<{url}|{title}>*"
        blocks.append({"type":"section","text":{"type":"mrkdwn","text":text}})
    return blocks

def post_to_slack(blocks):
    if not WEBHOOK:
        print("SLACK_WEBHOOK_URL not set")
        return False
    payload = {"text":"Surfit ì‹ ê·œ ì•„í‹°í´", "blocks": blocks}
    r = requests.post(WEBHOOK, json=payload, timeout=20)
    print("Slack:", r.status_code, r.text[:200])
    return r.ok

def main():
    ensure_cache()
    seen = load_seen()

    # 1) ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ë“¤ì—ì„œ í›„ë³´ ëª¨ìœ¼ê¸°
    candidates = []
    for lp in LIST_PAGES:
        try:
            res = fetch_html(lp)
            if res.status_code == 200:
                got = parse_list(res.text)
                candidates.extend(got)
        except Exception as e:
            print("list fetch error:", lp, e)
        time.sleep(0.5)

    # 2) ì‹ ê·œë§Œ ì¶”ë¦¬ê¸° (ë³´ì—¬ì¤€ ì  ì—†ëŠ” /article/ID)
    new_items = pick_new_items(candidates, seen)

    # 3) ì—†ìœ¼ë©´ ìƒìœ„ 5ê°œë¼ë„ ë³´ë‚´ê¸°(ìµœì´ˆ ì„¸íŒ… ì‹œ ìœ ìš©)
    if not new_items:
        print("No new items. Sending top picks for today.")
        # ìƒìœ„ 5ê°œì— ëŒ€í•´ ì„ì‹œ id ê³„ì‚°
        top = []
        for t,u in candidates[:5]:
            top.append((t,u,article_id(u)))
        blocks = build_slack_blocks(top)
        ok = post_to_slack(blocks)
        if ok:
            # ë³¸ë¬¸ì€ ë³´ë‚´ì§€ ì•Šì§€ë§Œ, ìºì‹œëŠ” ì—…ë°ì´íŠ¸ ì•ˆ í•¨(ì§„ì§œ ì‹ ê·œë§Œ ìºì‹œ)
            pass
        return

    # 4) ìŠ¬ë™ ì „ì†¡
    blocks = build_slack_blocks(new_items)
    ok = post_to_slack(blocks)

    # 5) ì „ì†¡ ì„±ê³µ ì‹œ ìºì‹œ ì—…ë°ì´íŠ¸ í›„ ì €ì¥
    if ok:
        for _, _, aid in new_items:
            seen.add(aid)
        save_seen(seen)

if __name__ == "__main__":
    main()
