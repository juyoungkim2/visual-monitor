import os, json, time, re
import datetime as dt
from pathlib import Path
from urllib.parse import urljoin
import requests
from bs4 import BeautifulSoup

SURFIT_BASE = "https://www.surfit.io"
LIST_PAGES = [
    "https://www.surfit.io/",
    "https://www.surfit.io/discover",
]
WEBHOOK = os.environ.get("SLACK_WEBHOOK_URL")
CACHE = Path(".cache/surfit_seen.json")

MAX_ITEMS = 8         # í•œ ë²ˆì— ì „ì†¡í•  ê°œìˆ˜
FETCH_TIMEOUT = 15
HEADERS = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) SurfitSlackBot/1.1",
    "Accept-Language": "ko-KR,ko;q=0.9,en;q=0.8",
}

def ensure_cache():
    CACHE.parent.mkdir(parents=True, exist_ok=True)
    if not CACHE.exists():
        CACHE.write_text("[]", encoding="utf-8")

def fetch(url):
    return requests.get(url, headers=HEADERS, timeout=FETCH_TIMEOUT)

def article_id(url: str) -> str:
    m = re.search(r"/article/([A-Za-z0-9\-_]+)", url)
    return m.group(1) if m else url

def load_seen():
    try:
        return set(json.loads(CACHE.read_text(encoding="utf-8")))
    except Exception:
        return set()

def save_seen(seen):
    # ìµœê·¼ 500ê°œë§Œ ìœ ì§€
    CACHE.write_text(json.dumps(sorted(list(seen))[-500:],
                                ensure_ascii=False, indent=2), encoding="utf-8")

def extract_article_urls_from_html(html: str):
    """
    JS ë Œë”ë§ ì—†ì´ ì›ë¬¸ ë¬¸ìì—´ì—ì„œ /article/ ë§í¬ë¥¼ ì •ê·œì‹ìœ¼ë¡œ ìˆ˜ì§‘
    ì ˆëŒ€/ìƒëŒ€ ê²½ë¡œ ëª¨ë‘ ì²˜ë¦¬
    """
    urls = set()

    # 1) ì ˆëŒ€ ê²½ë¡œ
    for u in re.findall(r"https?://www\.surfit\.io/article/[A-Za-z0-9\-_]+", html):
        urls.add(u)

    # 2) ìƒëŒ€ ê²½ë¡œ
    for u in re.findall(r'"(/article/[A-Za-z0-9\-_]+)"', html):
        urls.add(urljoin(SURFIT_BASE, u))

    return list(urls)

def parse_meta(url: str):
    """
    ê°œë³„ ê¸€ í˜ì´ì§€ì—ì„œ og:title / og:descriptionì„ ì½ì–´ ìŠ¬ë™ìš© í…ìŠ¤íŠ¸ë¥¼ ë§Œë“ ë‹¤.
    """
    try:
        r = fetch(url)
        if r.status_code != 200:
            return None, None
        soup = BeautifulSoup(r.text, "html.parser")

        title = None
        desc = None

        ogt = soup.find("meta", attrs={"property": "og:title"})
        if ogt and ogt.get("content"):
            title = ogt.get("content").strip()
        if not title and soup.title and soup.title.text:
            title = soup.title.text.strip()

        ogd = soup.find("meta", attrs={"property": "og:description"})
        if ogd and ogd.get("content"):
            desc = ogd.get("content").strip()
        if not desc:
            md = soup.find("meta", attrs={"name": "description"})
            if md and md.get("content"):
                desc = md.get("content").strip()

        if desc:
            desc = " ".join(desc.split())[:280]

        return title, desc
    except Exception:
        return None, None

def build_blocks(items):
    today = dt.datetime.now(dt.timezone(dt.timedelta(hours=9))).strftime("%Y-%m-%d")
    blocks = [{
        "type": "header",
        "text": {"type": "plain_text", "text": f"ğŸ§© Surfit ì‹ ê·œ ì•„í‹°í´ - {today}"}
    }]

    for url in items[:MAX_ITEMS]:
        title, desc = parse_meta(url)
        # ì œëª©ì´ ì—†ìœ¼ë©´ URLì—ì„œë¼ë„ ì•„ì´ë””ë¥¼ ì œëª©ì²˜ëŸ¼ ë³´ì—¬ì£¼ì
        if not title:
            title = f"Surfit Article {article_id(url)}"
        if desc:
            text = f"*<{url}|{title}>*\n{desc}"
        else:
            text = f"*<{url}|{title}>*"
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": text}})

    return blocks

def post_to_slack(blocks):
    if not WEBHOOK:
        print("SLACK_WEBHOOK_URL not set")
        return False
    payload = {"text": "Surfit ì‹ ê·œ ì•„í‹°í´", "blocks": blocks}
    r = requests.post(WEBHOOK, json=payload, timeout=20)
    print("Slack:", r.status_code, r.text[:200])
    return r.ok

def main():
    ensure_cache()
    seen = load_seen()

    # 1) ë¦¬ìŠ¤íŠ¸ í˜ì´ì§€ì—ì„œ ì›ë¬¸ ë¬¸ìì—´ì„ ê°€ì ¸ì™€ ë§í¬ ìˆ˜ì§‘
    candidates = []
    for page in LIST_PAGES:
        try:
            res = fetch(page)
            if res.status_code == 200 and res.text:
                urls = extract_article_urls_from_html(res.text)
                candidates.extend(urls)
        except Exception as e:
            print("fetch error:", page, e)
        time.sleep(0.5)

    # ì¤‘ë³µ ì œê±° ë° ìµœì‹  ìš°ì„ (ì•ìª½ ìœ ì§€)
    uniq = []
    s = set()
    for u in candidates:
        if u not in s:
            s.add(u)
            uniq.append(u)

    # 2) ì‹ ê·œë§Œ í•„í„°
    new_urls = [u for u in uniq if article_id(u) not in seen]

    # 3) ì‹ ê·œ ì—†ìœ¼ë©´ ìƒìœ„ 5ê°œë¼ë„ ì „ì†¡(ì´ˆê¸° ì„¸íŒ… í™•ì¸ìš©)
    send_urls = new_urls if new_urls else uniq[:5]
    if not send_urls:
        print("No article URLs found on list pages.")
        return

    blocks = build_blocks(send_urls)
    ok = post_to_slack(blocks)

    if ok:
        for u in new_urls:
            seen.add(article_id(u))
        save_seen(seen)

if __name__ == "__main__":
    main()
